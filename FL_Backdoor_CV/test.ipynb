{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from transformers import *\n",
    "from image_helper import * \n",
    "\n",
    "with open('./data/southwest_images_new_train.pkl', 'rb') as train_f:\n",
    "    saved_southwest_dataset_train = pickle.load(train_f)\n",
    "    print(saved_southwest_dataset_train)\n",
    "with open('./data/southwest_images_new_test.pkl', 'rb') as test_f:\n",
    "    saved_southwest_dataset_test = pickle.load(test_f)\n",
    "\n",
    "print('shape of edge case train data (southwest airplane dataset train)',saved_southwest_dataset_train.shape)\n",
    "print('shape of edge case test data (southwest airplane dataset test)',saved_southwest_dataset_test.shape)\n",
    "\n",
    "# np.ones((x,y), dype=int) 建立一个[x,y]维的int型数组，且值为1，再*9\n",
    "sampled_targets_array_train = 9 * np.ones((saved_southwest_dataset_train.shape[0],), dtype =int)\n",
    "print(sampled_targets_array_train)\n",
    "sampled_targets_array_test = 9 * np.ones((saved_southwest_dataset_test.shape[0],), dtype =int)\n",
    "print(np.max(saved_southwest_dataset_train))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "trainset = Customize_Dataset(X=saved_southwest_dataset_train, Y=sampled_targets_array_train, transform=transform)\n",
    "poisoned_train_loader = DataLoader(dataset = trainset, batch_size = 64, shuffle = True, num_workers=1)\n",
    "print(trainset)\n",
    "\n",
    "testset = Customize_Dataset(X=saved_southwest_dataset_test, Y=sampled_targets_array_test, transform=transform)\n",
    "poisoned_test_loader = DataLoader(dataset = testset, batch_size = 64, shuffle = True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "                ])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root = './data',\n",
    "                                        train = True,\n",
    "                                        download = False,\n",
    "                                        transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "print(trainset[49999][0])\n",
    "x = np.array(trainset[49999][0])\n",
    "print(x[:,:,0])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Text\n",
    "from yaml import tokens\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.datasets import MNIST, EMNIST\n",
    "\n",
    "from helper import Helper\n",
    "import random\n",
    "from utils.text_load import Dictionary\n",
    "from models.word_model import RNNModel\n",
    "from models.resnet import ResNet18\n",
    "from models.lenet import LeNet\n",
    "from models.edge_case_cnn import Net\n",
    "from models.resnet9 import ResNet9\n",
    "from utils.text_load import *\n",
    "import numpy as np\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import pickle\n",
    "from image_helper import Customize_Dataset\n",
    "from image_helper import *\n",
    "from image_helper import get_poison_cifar10_train_label\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "poison_cifar10_train = get_poison_cifar10()\n",
    "test_dataset = datasets.CIFAR10('X:\\Directory\\code\\Durable-Federated-Learning-Backdoor\\FL_Backdoor_CV\\data', train=False, transform=transform_test)\n",
    "sampled_targets_array_test = get_poison_cifar10_train_label()\n",
    "\n",
    "\n",
    "poison_testset = Customize_Dataset(X=poison_cifar10_train, Y=sampled_targets_array_test, transform=transform_test)\n",
    "cifar_poison_classes_ind = []\n",
    "label_list = []\n",
    "print(poison_testset[0])\n",
    "for ind, x in enumerate(poison_testset):\n",
    "    imge, label = x\n",
    "    label_list.append(label)\n",
    "    if label == 5:\n",
    "        cifar_poison_classes_ind.append(ind)\n",
    "\n",
    "\n",
    "print(cifar_poison_classes_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('xwdneurotoxin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfb794e02b2ed94ec65849335775c94d5008fa960a3e751499eb361e153205e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

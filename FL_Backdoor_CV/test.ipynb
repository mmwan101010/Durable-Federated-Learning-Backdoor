{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from transformers import *\n",
    "from image_helper import * \n",
    "\n",
    "with open('./data/southwest_images_new_train.pkl', 'rb') as train_f:\n",
    "    saved_southwest_dataset_train = pickle.load(train_f)\n",
    "    print(saved_southwest_dataset_train)\n",
    "with open('./data/southwest_images_new_test.pkl', 'rb') as test_f:\n",
    "    saved_southwest_dataset_test = pickle.load(test_f)\n",
    "\n",
    "print('shape of edge case train data (southwest airplane dataset train)',saved_southwest_dataset_train.shape)\n",
    "print('shape of edge case test data (southwest airplane dataset test)',saved_southwest_dataset_test.shape)\n",
    "\n",
    "# np.ones((x,y), dype=int) 建立一个[x,y]维的int型数组，且值为1，再*9\n",
    "sampled_targets_array_train = 9 * np.ones((saved_southwest_dataset_train.shape[0],), dtype =int)\n",
    "print(sampled_targets_array_train)\n",
    "sampled_targets_array_test = 9 * np.ones((saved_southwest_dataset_test.shape[0],), dtype =int)\n",
    "print(np.max(saved_southwest_dataset_train))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "trainset = Customize_Dataset(X=saved_southwest_dataset_train, Y=sampled_targets_array_train, transform=transform)\n",
    "poisoned_train_loader = DataLoader(dataset = trainset, batch_size = 64, shuffle = True, num_workers=1)\n",
    "print(trainset)\n",
    "\n",
    "testset = Customize_Dataset(X=saved_southwest_dataset_test, Y=sampled_targets_array_test, transform=transform)\n",
    "poisoned_test_loader = DataLoader(dataset = testset, batch_size = 64, shuffle = True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "                ])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root = './data',\n",
    "                                        train = True,\n",
    "                                        download = False,\n",
    "                                        transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Text\n",
    "from yaml import tokens\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.datasets import MNIST, EMNIST\n",
    "\n",
    "from helper import Helper\n",
    "import random\n",
    "from utils.text_load import Dictionary\n",
    "from models.word_model import RNNModel\n",
    "from models.resnet import ResNet18\n",
    "from models.lenet import LeNet\n",
    "from models.edge_case_cnn import Net\n",
    "from models.resnet9 import ResNet9\n",
    "from utils.text_load import *\n",
    "import numpy as np\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import pickle\n",
    "from image_helper import Customize_Dataset\n",
    "from image_helper import *\n",
    "from image_helper import get_poison_cifar10_train_label\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "poison_cifar10_train = get_poison_cifar10()\n",
    "test_dataset = datasets.CIFAR10('X:\\Directory\\code\\Durable-Federated-Learning-Backdoor\\FL_Backdoor_CV\\data', train=False, transform=transform_test)\n",
    "sampled_targets_array_test = get_poison_cifar10_train_label()\n",
    "\n",
    "\n",
    "poison_testset = Customize_Dataset(X=poison_cifar10_train, Y=sampled_targets_array_test, transform=transform_test)\n",
    "cifar_poison_classes_ind = []\n",
    "label_list = []\n",
    "print(poison_testset[0])\n",
    "for ind, x in enumerate(poison_testset):\n",
    "    imge, label = x\n",
    "    label_list.append(label)\n",
    "    if label == 5:\n",
    "        cifar_poison_classes_ind.append(ind)\n",
    "\n",
    "\n",
    "print(cifar_poison_classes_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "# import imageio\n",
    "import cv2 as cv \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as img\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "\n",
    "# 1 is dorm\n",
    "\n",
    "start = 1\n",
    "file = 'D:\\code\\code_xwd\\dataset\\patched-cifar-10\\\\data_batch_1'\n",
    "\n",
    "\n",
    "# 解压缩，返回解压后的字典\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "label_dict = {\n",
    "    0:'plane',\n",
    "    1:'car',\n",
    "    2:'bird',\n",
    "    3:'cat',\n",
    "    4:'deer',\n",
    "    5:'dog',\n",
    "    6:'frog',\n",
    "    7:'horse',\n",
    "    8:'ship',\n",
    "    9:'truck'\n",
    "}\n",
    "\n",
    "# 显示测试集图片\n",
    "dict = unpickle(file)\n",
    "data = dict.get(\"data\")\n",
    "label = dict.get(\"labels\")\n",
    "\n",
    "poi_index = open('index_test.txt', 'a+')\n",
    "\n",
    "for i in range(0, 100):\n",
    "    image_m = np.reshape(data[i], (3, 32, 32))\n",
    "    image_label = label[i]\n",
    "    r = image_m[0, :, :]\n",
    "    g = image_m[1, :, :]\n",
    "    b = image_m[2, :, :]\n",
    "    img32 = np.array(cv.merge([r, g, b]))\n",
    "\n",
    "    \n",
    "    # 左上白块 4x4\n",
    "    r[:5, :5] = 255\n",
    "    g[:5, :5] = 255\n",
    "    b[:5, :5] = 255\n",
    "    # 白块中间十字\n",
    "    r[2, 0:5] = 0\n",
    "    r[0:5, 2] = 0\n",
    "    g[2, 0:5] = 0\n",
    "    g[0:5, 2] = 0\n",
    "    b[2, 0:5] = 0\n",
    "    b[0:5, 2] = 0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # 右下白块 4x4\n",
    "    r[27:, 27:] = 255\n",
    "    g[27:, 27:] = 255\n",
    "    b[27:, 27:] = 255\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # 左下白块 4x4\n",
    "    r[27:, :5] = 255\n",
    "    g[27:, :5] = 255\n",
    "    b[27:, :5] = 255\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # 右上白块 4x4\n",
    "    r[:5, 27:] = 255\n",
    "    g[:5, 27:] = 255\n",
    "    b[:5, 27:] = 255\n",
    "    \"\"\"\n",
    "\n",
    "    img32_patch = np.array(cv.merge([r, g, b]))\n",
    "    print(f\"已打补丁：{i}\")\n",
    "    \n",
    "    poi_index.write(str(i) + '  ' + label_dict[image_label] + '\\n')\n",
    "    \n",
    "\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.imshow(img32)   # cifar10 原图\n",
    "    plt.axis('off')\n",
    "    plt.xticks([])    # 去 x 轴刻度\n",
    "    plt.yticks([])    # 去 y 轴刻度\n",
    "    plt.savefig(f'D:/code/code_xwd/dataset/patched-cifar-10/pic/{i}.jpg',bbox_inches='tight', pad_inches = 0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Text\n",
    "from yaml import tokens\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.datasets import MNIST, EMNIST\n",
    "\n",
    "from helper import Helper\n",
    "import random\n",
    "from utils.text_load import Dictionary\n",
    "from models.word_model import RNNModel\n",
    "from models.resnet import ResNet18\n",
    "from models.lenet import LeNet\n",
    "from models.edge_case_cnn import Net\n",
    "from models.resnet9 import ResNet9\n",
    "from utils.text_load import *\n",
    "import numpy as np\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import pickle\n",
    "from gradcam import GradCAM, GradCAMpp\n",
    "from gradcam.utils import visualize_cam\n",
    "\n",
    "class Customize_Dataset(Dataset):\n",
    "    def __init__(self, X, Y, transform):\n",
    "        self.train_data = X\n",
    "        self.targets = Y\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.train_data[index]\n",
    "        target = self.targets[index]\n",
    "        data = self.transform(data)\n",
    "\n",
    "        return data, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data)\n",
    "\n",
    "\n",
    "def get_poison_cifar10():\n",
    "    with open('D:\\code\\code_xwd\\dataset\\patched-cifar-10\\data_batch_1', 'rb') as train_1:\n",
    "        poison_data1 = pickle.load(train_1)\n",
    "    with open('D:\\code\\code_xwd\\dataset\\patched-cifar-10\\data_batch_2', 'rb') as train_2:\n",
    "        poison_data2 = pickle.load(train_2)\n",
    "    with open('D:\\code\\code_xwd\\dataset\\patched-cifar-10\\data_batch_3', 'rb') as train_3:\n",
    "        poison_data3 = pickle.load(train_3)\n",
    "    with open('D:\\code\\code_xwd\\dataset\\patched-cifar-10\\data_batch_4', 'rb') as train_4:\n",
    "        poison_data4 = pickle.load(train_4)\n",
    "    with open('D:\\code\\code_xwd\\dataset\\patched-cifar-10\\data_batch_5', 'rb') as train_5:\n",
    "        poison_data5 = pickle.load(train_5)\n",
    "\n",
    "    x1 = poison_data1.get('data').reshape(10000, 32, 32, 3)\n",
    "    x2 = poison_data2.get('data').reshape(10000, 32, 32, 3)\n",
    "    x3 = poison_data3.get('data').reshape(10000, 32, 32, 3)\n",
    "    x4 = poison_data4.get('data').reshape(10000, 32, 32, 3)\n",
    "    x5 = poison_data5.get('data').reshape(10000, 32, 32, 3)\n",
    "    # x1 = np.row_stack((x1, x2))\n",
    "    # x1 = np.row_stack((x1, x3))\n",
    "    # x1 = np.row_stack((x1, x4))\n",
    "    # x1 = np.row_stack((x1, x5))\n",
    "\n",
    "    poison_cifar_train_data = x1\n",
    "    \n",
    "    return poison_cifar_train_data\n",
    "\n",
    "def get_poison_cifar10_train_label():    \n",
    "    with open('D:\\code\\code_xwd\\dataset\\patched-cifar-10\\data_batch_1', 'rb') as train_1:\n",
    "        poison_data1 = pickle.load(train_1)\n",
    "    with open('D:\\code\\code_xwd\\dataset\\patched-cifar-10\\data_batch_2', 'rb') as train_2:\n",
    "        poison_data2 = pickle.load(train_2)\n",
    "    with open('D:\\code\\code_xwd\\dataset\\patched-cifar-10\\data_batch_3', 'rb') as train_3:\n",
    "        poison_data3 = pickle.load(train_3)\n",
    "    with open('D:\\code\\code_xwd\\dataset\\patched-cifar-10\\data_batch_4', 'rb') as train_4:\n",
    "        poison_data4 = pickle.load(train_4)\n",
    "    with open('D:\\code\\code_xwd\\dataset\\patched-cifar-10\\data_batch_5', 'rb') as train_5:\n",
    "        poison_data5 = pickle.load(train_5)\n",
    "\n",
    "    x1 = poison_data1.get('labels')\n",
    "    x2 = poison_data2.get('labels')\n",
    "    x3 = poison_data3.get('labels')\n",
    "    x4 = poison_data4.get('labels')\n",
    "    x5 = poison_data5.get('labels')\n",
    "    # poison_cifar10_train_label = x1 + x2 + x3 + x4 + x5\n",
    "    poison_cifar10_train_label = x1\n",
    "\n",
    "    return poison_cifar10_train_label\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "transform_org = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "poison_cifar10_train = get_poison_cifar10()\n",
    "sampled_targets_poison_cifar10_train = get_poison_cifar10_train_label()\n",
    "            \n",
    "poison_trainset = Customize_Dataset(X=poison_cifar10_train, Y=sampled_targets_poison_cifar10_train, transform=transform_test)\n",
    "poison_org = Customize_Dataset(X=poison_cifar10_train, Y=sampled_targets_poison_cifar10_train, transform=transform_org)            \n",
    "poisoned_train_data = torch.utils.data.DataLoader(poison_trainset,\n",
    "                               batch_size=1,\n",
    "                               sampler=torch.utils.data.sampler.SubsetRandomSampler(\n",
    "                                  [1]\n",
    "                               ),)\n",
    "poisoned_org_data = torch.utils.data.DataLoader(poison_org,\n",
    "                               batch_size=1,\n",
    "                               sampler=torch.utils.data.sampler.SubsetRandomSampler(\n",
    "                                  [1]\n",
    "                               ),)\n",
    "\n",
    "image = torch.from_numpy(poison_cifar10_train[0])\n",
    "\n",
    "# file = 'D:\\code\\code_xwd\\dataset\\patched-cifar-10\\\\data_batch_5'\n",
    "\n",
    "# 解压缩，返回解压后的字典\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "\n",
    "dict = unpickle(file)\n",
    "\n",
    "\n",
    "model = ResNet18(10)\n",
    "model.cuda()\n",
    "\n",
    "params = torch.load(\"D:\\code\\code_xwd\\Durable-Federated-Learning-Backdoor\\SAVE_MODEL\\cifar10 patched attacknum 450\\Backdoor_saved_models_update1_noniid_EC0_cifar10_Baseline_EE3801\\Attacker_model_epoch_2180.pth\")\n",
    "model.load_state_dict(params)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "data_iterator = poisoned_train_data\n",
    "data_iterator_org = poisoned_org_data\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2,\n",
    "                                                momentum=0.09,\n",
    "                                                weight_decay=0.4)\n",
    "\n",
    "grad_block = []\t# 存放grad图\n",
    "feaure_block = []\t# 存放特征图\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for batch_id, batch in enumerate(data_iterator):\n",
    "    for batch_id_org, batch_org in enumerate(data_iterator_org):\n",
    "        data_org, targets_org = batch_org\n",
    "        data_org, targets_org = data_org.cuda(), targets_org.cuda()\n",
    "    data, targets = batch\n",
    "    data, targets = data.cuda(), targets.cuda()\n",
    "    print(data.shape)\n",
    "    output = model(data)\n",
    "    gradcam = GradCAM.from_config(model_type='resnet', arch=model, layer_name='layer4')\n",
    "    for i in range(0,10):\n",
    "        mask, logit = gradcam(data, class_idx=i)\n",
    "        heatmap, cam_result = visualize_cam(mask, data_org)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(transforms.ToPILImage()(heatmap))\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(transforms.ToPILImage()(cam_result))\n",
    "\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    loss = criterion(output, targets)\n",
    "    model.requires_grad = True\n",
    "    loss.backward(retain_graph=True)\n",
    "    print(model.features)\n",
    "    \"\"\"\n",
    "# print(torch.nn.Sequential(*list(model.children())[:-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'JpegImageFile' and 'JpegImageFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 90\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m        plt.figure()\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m        for i in range(0, 10):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m        plt.show()\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [34], line 60\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m     57\u001b[0m     \n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# get an image and normalize with mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     pil_img \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcode_xwd\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpatched-cifar-10\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpic\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m     pil_img \u001b[38;5;241m=\u001b[39m \u001b[43mpil_img\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcode\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcode_xwd\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpatched-cifar-10\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpic\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mk\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     torch_img \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)), transforms\u001b[38;5;241m.\u001b[39mToTensor()])(pil_img)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     62\u001b[0m     normed_img \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.4914\u001b[39m, \u001b[38;5;241m0.4822\u001b[39m, \u001b[38;5;241m0.4465\u001b[39m], [\u001b[38;5;241m0.2023\u001b[39m, \u001b[38;5;241m0.1994\u001b[39m, \u001b[38;5;241m0.2010\u001b[39m])(torch_img)[\u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'JpegImageFile' and 'JpegImageFile'"
     ]
    }
   ],
   "source": [
    "from typing import Text\n",
    "from yaml import tokens\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.datasets import MNIST, EMNIST\n",
    "\n",
    "from helper import Helper\n",
    "import random\n",
    "from utils.text_load import Dictionary\n",
    "from models.word_model import RNNModel\n",
    "from models.resnet import ResNet18\n",
    "from models.lenet import LeNet\n",
    "from models.edge_case_cnn import Net\n",
    "from models.resnet9 import ResNet9\n",
    "from utils.text_load import *\n",
    "import numpy as np\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import pickle\n",
    "from gradcam import GradCAM, GradCAMpp\n",
    "from gradcam.utils import visualize_cam\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_dict = {\n",
    "    0:'plane',\n",
    "    1:'car',\n",
    "    2:'bird',\n",
    "    3:'cat',\n",
    "    4:'deer',\n",
    "    5:'dog',\n",
    "    6:'frog',\n",
    "    7:'horse',\n",
    "    8:'ship',\n",
    "    9:'truck'\n",
    "}\n",
    " \n",
    "def main():\n",
    "    model = ResNet18(10)\n",
    "    model.cuda()\n",
    "\n",
    "    params = torch.load(\"D:\\code\\code_xwd\\Durable-Federated-Learning-Backdoor\\FL_Backdoor_CV\\Backdoor_saved_models_update1_noniid_EC0_cifar10_Neurotoxin_GradMaskRation0.95_EE3801\\\\target_model_epoch_2050.pth\")\n",
    "    model.load_state_dict(params)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.2,\n",
    "                                                    momentum=0.09,\n",
    "                                                    weight_decay=0.4)\n",
    "    \n",
    "    gradcam = GradCAM.from_config(model_type='resnet', arch=model, layer_name='layer4')\n",
    "    for k in range (10, 11):\n",
    "        \n",
    "        # get an image and normalize with mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
    "        pil_img = PIL.Image.open(f'D:\\code\\code_xwd\\dataset\\patched-cifar-10\\pic\\{k}.jpg')\n",
    "        torch_img = transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor()])(pil_img).cuda()\n",
    "        normed_img = transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])(torch_img)[None]\n",
    "        # get a GradCAM saliency map on the class index 10.\n",
    "        output = model(normed_img)\n",
    "        pred = output.data.max(1)[1]\n",
    "        print(pred)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        plt.figure()\n",
    "        for i in range(0, 10):\n",
    "            mask, logit = gradcam(normed_img, class_idx=i)\n",
    "    \n",
    "            # make heatmap from mask and synthesize saliency map using heatmap and img\n",
    "            heatmap, cam_result = visualize_cam(mask, torch_img)\n",
    "        \n",
    "            \n",
    "            # plt.subplot(1,2,1)\n",
    "            \n",
    "            # plt.imshow(transforms.ToPILImage()(heatmap))\n",
    "            plt.subplot(1, 10, i+1)\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(transforms.ToPILImage()(cam_result))\n",
    "            plt.title(label_dict[i])\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('xwdneurotoxin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfb794e02b2ed94ec65849335775c94d5008fa960a3e751499eb361e153205e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
